{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshanananth/LINE_DETECTION/blob/main/gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1) Install dependencies (quiet)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install kornia kornia-rs opencv-python matplotlib networkx\n",
        "\n",
        "import sys, platform, torch\n",
        "print(f\"Python {sys.version.split()[0]} | PyTorch {torch.__version__} | CUDA? {torch.cuda.is_available()} | {platform.platform()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Y85bDp7bC1",
        "outputId": "3ae3dc3c-aa97-4bac-dede-644ef80144fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPython 3.12.12 | PyTorch 2.10.0+cpu | CUDA? False | Linux-6.6.113+-x86_64-with-glibc2.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2) Imports & helper functions (I/O, drawing, geometry)\n",
        "from typing import List, Tuple, Dict\n",
        "import cv2, json, math, numpy as np\n",
        "import torch\n",
        "import kornia as K\n",
        "import kornia.feature as KF\n",
        "import networkx as nx\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_image_rgba(path: str):\n",
        "    \"\"\"\n",
        "    Read an image with OpenCV (BGR), convert to RGB float32 in [0,1].\n",
        "    \"\"\"\n",
        "    bgr = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    if bgr is None:\n",
        "        raise FileNotFoundError(f\"Could not read: {path}\")\n",
        "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb_f = (rgb.astype(np.float32) / 255.0)\n",
        "    return rgb_f, bgr  # rgb_f for Kornia, bgr for OpenCV drawing if needed\n",
        "\n",
        "def prepare_for_sold2(rgb_f: np.ndarray, target_max_side: int = 800):\n",
        "    \"\"\"\n",
        "    Always return a torch tensor T with shape [1,1,H,W], float32 in [0,1],\n",
        "    optionally resized so max(H,W) <= target_max_side.\n",
        "\n",
        "    Notes:\n",
        "    - SOLD² (config=None) is tuned for ~300–800 px per side for best OOTB results.  # [3](https://github.com/cvg/SOLD2)\n",
        "    - Kornia/SOLD² expects a grayscale batch [B,1,H,W].                             # [1](https://www.kornia.org/tutorials/nbs/line_detection_and_matching_sold2.html)[2](https://kornia.readthedocs.io/en/latest/models/sold2.html)\n",
        "    \"\"\"\n",
        "    H, W = rgb_f.shape[:2]\n",
        "    scale = 1.0\n",
        "    if max(H, W) > target_max_side:\n",
        "        scale = target_max_side / max(H, W)\n",
        "        new_h, new_w = int(round(H * scale)), int(round(W * scale))\n",
        "        rgb_f = cv2.resize(rgb_f, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Convert to tensor; handle both RGB [H,W,3] and grayscale [H,W].\n",
        "    t = K.image_to_tensor(rgb_f, keepdim=False)  # -> [C,H,W] OR [1, H, W] if grayscale\n",
        "    if t.dim() == 3:                              # [C,H,W] -> [1,C,H,W]\n",
        "        t = t.unsqueeze(0)\n",
        "    elif t.dim() == 4:\n",
        "        pass  # already [B,C,H,W]\n",
        "    else:\n",
        "        raise RuntimeError(f\"Unexpected tensor dims: {t.shape}\")\n",
        "\n",
        "    # Ensure 1 channel (grayscale)\n",
        "    if t.shape[1] == 1:\n",
        "        # already grayscale [B,1,H,W]\n",
        "        pass\n",
        "    elif t.shape[1] == 3:\n",
        "        t = K.color.rgb_to_grayscale(t)          # -> [B,1,H,W]\n",
        "    else:\n",
        "        # If unusual number of channels, reduce to 1 via mean as a fallback.\n",
        "        t = t.mean(1, keepdim=True)\n",
        "\n",
        "    # Final safety check: [B,1,H,W]\n",
        "    if not (t.dim() == 4 and t.shape[1] == 1):\n",
        "        raise RuntimeError(f\"Expected [B,1,H,W], got {t.shape}\")\n",
        "\n",
        "    return t, scale, rgb_f  # return the (possibly resized) RGB float image too\n",
        "\n",
        "def ij_to_xy(lines_ij: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Kornia/SOLD² returns line segments as (N,2,2) in ij order (row, col).  # [3](https://www.kornia.org/tutorials/nbs/line_detection_and_matching_sold2.html)\n",
        "    Convert to xy for OpenCV drawing.\n",
        "    \"\"\"\n",
        "    l = lines_ij\n",
        "    xy = np.zeros_like(l)\n",
        "    xy[:,0,0] = l[:,0,1]; xy[:,0,1] = l[:,0,0]  # (x0,y0) = (j0,i0)\n",
        "    xy[:,1,0] = l[:,1,1]; xy[:,1,1] = l[:,1,0]  # (x1,y1) = (j1,i1)\n",
        "    return xy\n",
        "\n",
        "def draw_lines_on_bgr(bgr: np.ndarray, lines_xy: np.ndarray, color=(0,0,255), thickness=2):\n",
        "    out = bgr.copy()\n",
        "    for seg in lines_xy:\n",
        "        (x0,y0),(x1,y1) = seg.astype(int)\n",
        "        cv2.line(out, (x0,y0), (x1,y1), color, thickness, cv2.LINE_AA)\n",
        "    return out\n",
        "\n",
        "def save_json_lines(path: str, lines_xy: np.ndarray, scale_back: float = 1.0):\n",
        "    \"\"\"\n",
        "    Save endpoints to JSON: [{\"x1\":..,\"y1\":..,\"x2\":..,\"y2\":..}, ...]\n",
        "    If you resized for inference, set scale_back=1/scale to map back to the original image size.\n",
        "    \"\"\"\n",
        "    sb = float(scale_back)\n",
        "    items = [{\"x1\": float(seg[0,0])*sb, \"y1\": float(seg[0,1])*sb,\n",
        "              \"x2\": float(seg[1,0])*sb, \"y2\": float(seg[1,1])*sb} for seg in lines_xy]\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump({\"lines\": items}, f, indent=2)\n",
        "\n",
        "def scale_lines(lines_xy: np.ndarray, factor: float) -> np.ndarray:\n",
        "    \"\"\"Scale line coordinates by a factor.\"\"\"\n",
        "    return lines_xy * float(factor)\n",
        "\n",
        "def snap_points(points: np.ndarray, eps: float = 5.0) -> np.ndarray:\n",
        "    \"\"\"Merge nearby points within eps (pixels); returns snapped (M,2) array.\"\"\"\n",
        "    if len(points) == 0:\n",
        "        return points\n",
        "    pts = points.copy()\n",
        "    used = np.zeros(len(pts), dtype=bool)\n",
        "    clusters = []\n",
        "    for i, p in enumerate(pts):\n",
        "        if used[i]: continue\n",
        "        cluster = [i]\n",
        "        for j in range(i+1, len(pts)):\n",
        "            if used[j]: continue\n",
        "            if np.linalg.norm(pts[j] - p) <= eps:\n",
        "                cluster.append(j); used[j] = True\n",
        "        used[i] = True\n",
        "        clusters.append(np.mean(pts[cluster], axis=0))\n",
        "    return np.array(clusters, dtype=np.float32)\n",
        "\n",
        "def build_connectivity_graph(lines_xy: np.ndarray, snap_eps: float = 5.0) -> nx.Graph:\n",
        "    \"\"\"Endpoints -> nodes (snapped); segments -> edges.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    if lines_xy.size == 0:\n",
        "        return G\n",
        "    endpoints = lines_xy.reshape(-1, 2)              # (2N,2)\n",
        "    snapped = snap_points(endpoints, eps=snap_eps)   # (M,2)\n",
        "\n",
        "    def nearest_idx(p):\n",
        "        d = np.linalg.norm(snapped - p, axis=1)\n",
        "        return int(np.argmin(d))\n",
        "\n",
        "    for idx, (x,y) in enumerate(snapped):\n",
        "        G.add_node(idx, x=float(x), y=float(y))\n",
        "    for (x0,y0),(x1,y1) in lines_xy:\n",
        "        u = nearest_idx(np.array([x0,y0], dtype=np.float32))\n",
        "        v = nearest_idx(np.array([x1,y1], dtype=np.float32))\n",
        "        if u != v:\n",
        "            G.add_edge(u, v)\n",
        "    return G\n",
        "\n",
        "def show_image(title, img_rgb):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RK97RpBu7biO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3) Load SOLD² (pretrained) — uses Kornia integration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "sold2 = KF.SOLD2(pretrained=True, config=None).to(device).eval()\n",
        "# Notes:\n",
        "# - SOLD² is integrated into Kornia and can be loaded with pretrained weights as above.  # [1](https://github.com/cvg/SOLD2)[2](https://kornia.readthedocs.io/en/latest/models/sold2.html)\n",
        "# - Kornia’s official tutorial uses this API and expects grayscale batches [B,1,H,W].      # [3](https://www.kornia.org/tutorials/nbs/line_detection_and_matching_sold2.html)"
      ],
      "metadata": {
        "id": "szsgquLK7tcY",
        "outputId": "c9a77432-d760-48a2-8d8b-d001927bce0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"http://cmp.felk.cvut.cz/~mishkdmy/models/sold2_wireframe.pth\" to /root/.cache/torch/hub/checkpoints/sold2_wireframe.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140M/140M [00:05<00:00, 26.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4) Quick demo on a sample image\n",
        "sample_url = \"https://github.com/cvg/SOLD2/raw/main/assets/images/terrace0.JPG\"  # sample from the repo  # [1](https://github.com/cvg/SOLD2)\n",
        "!wget -q -O /content/sample.jpg {sample_url}\n",
        "\n",
        "rgb_f, bgr = load_image_rgba(\"/content/sample.jpg\")\n",
        "t, scale, resized_rgb = prepare_for_sold2(rgb_f, target_max_side=800)\n",
        "\n",
        "# Sanity check: must be [1,1,H,W]\n",
        "print(\"Input tensor shape to SOLD²:\", t.shape)\n",
        "assert t.dim() == 4 and t.shape[1] == 1, \"Expected [B,1,H,W]\"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = sold2(t.to(device))\n",
        "\n",
        "# Extract lines (ij -> xy) for drawing\n",
        "line_segs_ij = outputs[\"line_segments\"][0].detach().cpu().numpy()  # (N,2,2) in ij  # [3](https://www.kornia.org/tutorials/nbs/line_detection_and_matching_sold2.html)\n",
        "lines_xy = ij_to_xy(line_segs_ij)\n",
        "\n",
        "# Draw overlay on the (possibly resized) image\n",
        "resized_bgr = cv2.cvtColor((resized_rgb*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "overlay_bgr = draw_lines_on_bgr(resized_bgr, lines_xy, (0,0,255), 2)\n",
        "overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "show_image(\"SOLD² detected lines (red overlay)\", overlay_rgb)\n",
        "\n",
        "# Save overlay and JSON (at resized scale)\n",
        "cv2.imwrite(\"/content/sample_sold2_overlay.png\", overlay_bgr)\n",
        "save_json_lines(\"/content/sample_sold2_lines.json\", lines_xy, scale_back=1.0)\n",
        "print(\"Saved:\", \"/content/sample_sold2_overlay.png\", \"and\", \"/content/sample_sold2_lines.json\")\n",
        "\n",
        "# (Optional) If you want lines at original resolution for downstream use:\n",
        "if scale != 1.0:\n",
        "    lines_xy_orig = scale_lines(lines_xy, factor=1.0/scale)\n",
        "    save_json_lines(\"/content/sample_sold2_lines_original_size.json\", lines_xy_orig, scale_back=1.0)\n",
        "    print(\"Also saved original-size coordinates:\", \"/content/sample_sold2_lines_original_size.json\")"
      ],
      "metadata": {
        "id": "VHbcIsme7v8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5) Run SOLD² on your own images (upload)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # select one or more diagram images\n",
        "\n",
        "results = []\n",
        "for fname in uploaded.keys():\n",
        "    rgb_f, bgr = load_image_rgba(fname)\n",
        "    t, scale, resized_rgb = prepare_for_sold2(rgb_f, target_max_side=1200)  # increase if lines are very thin\n",
        "    print(f\"\\n{fname}: tensor {t.shape}, scale={scale:.3f}\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = sold2(t.to(device))\n",
        "\n",
        "    line_segs_ij = outputs[\"line_segments\"][0].detach().cpu().numpy()\n",
        "    lines_xy = ij_to_xy(line_segs_ij)\n",
        "\n",
        "    # Overlay on resized image\n",
        "    resized_bgr = cv2.cvtColor((resized_rgb*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "    overlay_bgr = draw_lines_on_bgr(resized_bgr, lines_xy, (0,0,255), 2)\n",
        "    overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "    show_image(f\"{fname} — SOLD² overlay\", overlay_rgb)\n",
        "\n",
        "    out_png  = f\"/content/{Path(fname).stem}_sold2_overlay.png\"\n",
        "    out_json = f\"/content/{Path(fname).stem}_sold2_lines.json\"\n",
        "    cv2.imwrite(out_png, overlay_bgr)\n",
        "    save_json_lines(out_json, lines_xy, scale_back=1.0)\n",
        "\n",
        "    # (Optional) also export original-resolution coordinates\n",
        "    if scale != 1.0:\n",
        "        out_json_orig = f\"/content/{Path(fname).stem}_sold2_lines_original_size.json\"\n",
        "        save_json_lines(out_json_orig, scale_lines(lines_xy, 1.0/scale), scale_back=1.0)\n",
        "\n",
        "    results.append((out_png, out_json))\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "eNGeWc6y7ypK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6) (Optional) Build a simple connectivity graph for the last image\n",
        "# Treat each segment’s endpoints as nodes (snapped), segments as edges.\n",
        "# You can later connect detected symbols to the nearest node.\n",
        "\n",
        "# If you just ran cell 5, 'lines_xy' refers to the last processed file.\n",
        "G = build_connectivity_graph(lines_xy, snap_eps=5.0)\n",
        "print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "# Visualize graph on a blank canvas (same size as resized image used for overlay)\n",
        "H, W = overlay_rgb.shape[:2]\n",
        "canvas = np.ones((H, W, 3), dtype=np.uint8) * 255\n",
        "for (u, v) in G.edges():\n",
        "    x0, y0 = int(G.nodes[u]['x']), int(G.nodes[u]['y'])\n",
        "    x1, y1 = int(G.nodes[v]['x']), int(G.nodes[v]['y'])\n",
        "    cv2.line(canvas, (x0,y0), (x1,y1), (0,0,0), 1, cv2.LINE_AA)\n",
        "for n in G.nodes():\n",
        "    x, y = int(G.nodes[n]['x']), int(G.nodes[n]['y'])\n",
        "    cv2.circle(canvas, (x,y), 3, (0,0,255), -1, cv2.LINE_AA)\n",
        "\n",
        "show_image(\"Connectivity Graph (nodes + edges)\", cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Export graph as JSON\n",
        "graph_path = \"/content/sold2_graph.json\"\n",
        "with open(graph_path, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"nodes\": [{\"id\": int(n), \"x\": float(G.nodes[n][\"x\"]), \"y\": float(G.nodes[n][\"y\"])} for n in G.nodes()],\n",
        "        \"edges\": [{\"u\": int(u), \"v\": int(v)} for u, v in G.edges()]\n",
        "    }, f, indent=2)\n",
        "print(\"Saved graph to:\", graph_path)"
      ],
      "metadata": {
        "id": "sxaIryAZ73UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Auq_ltNb77AT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}